# TensorFlowLite-workflow
Model shrinking using TFLite
In this repo, two approaches to downsize a neural network are practiced in the provided notebooks.  The ultimate goal is to shrink a model so small that it can fit an IoT device and run inference with very low latency.  TensorFlow Lite Framework provide tools and processes to achieve this.  The main technique is quantization to reduce the size of the model.  Quantization can be performed in two process: (1) training the neural network with constrains of generating only integer-like weights and activations, which is also refered to as quantization-aware training, and (2) quantizing the paramters (weights and activations) after the model is trained and ready for use, this is refered to as post-quantization training. There are two notebook in this repo that use each technique to down-size the model that is trained to detect a failure using the sound of an engine (data not provided) with a failure component.  Using TensorFlow Lite framework the model is downsized to more than 4 times smaller and the performance of the shrinked model could be verified using the interpreter.  This is a good practice to verify the model before deploying it to the device for inference.  Although in many applications, reduction in accuracy after down-sizing could be expected, the size of the model should still be much smaller.  
